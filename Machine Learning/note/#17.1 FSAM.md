# Forward Stagewise Additive Modeling(FSAM)

> Greedily fit one function at a time without adjusting previous functions, hence “forward stagewise”.

$$
f_m = f_{m-1}+v_mh_m
$$

in Objective function
$$
(v_m, h_m) = argmin\frac{1}{n}\sum^n_{i=1}\ell(y_i, f_{m-1}(x_i)+vh(x_i))
$$

## AdaBoost

> Gradient Boost with exp loss

$$
\begin{align}
(v_m, h_m) 
& = argmin\frac{1}{n}\sum^n_{i=1}\ell_{exp}[y_i, f_{m-1}(x_i)+vh(x_i)]\\
& = w_i^mexp[-y_ivh(x_i)]
\end{align}
$$

Note:

- $w_i^m = exp[-y_if_{m-1}(x_i)]$

- $f_{m-1}(x_i)$ is constant

$$
\begin{align}
w_i^{m+1} = 
& = exp[-y_ifm(x_i)] \\
& = w_i^mexp[2v_m]
\end{align}
$$

==23/54== 11.pdf

### Exp loss

- Exponential loss puts a high penalty on misclassified examples.
  =) not robust to outliers / noise.

- Empirically, AdaBoost has degraded performance in situations with high Bayes error rate (intrinsic randomness in the label)